{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TncgOmuZ-13m",
        "outputId": "7f25e351-807d-4370-9cda-7a5241caa7fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-gpu==2.9.1\n",
            "  Downloading tensorflow_gpu-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (1.6.3)\n",
            "Collecting flatbuffers<2,>=1.12 (from tensorflow-gpu==2.9.1)\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-gpu==2.9.1)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (1.60.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (3.9.0)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0 (from tensorflow-gpu==2.9.1)\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.1.1 (from tensorflow-gpu==2.9.1)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (16.0.6)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (23.2)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow-gpu==2.9.1)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (1.16.0)\n",
            "Collecting tensorboard<2.10,>=2.9 (from tensorflow-gpu==2.9.1)\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (0.34.0)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow-gpu==2.9.1)\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.1) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow-gpu==2.9.1) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (2.17.3)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.1) (3.2.2)\n",
            "Installing collected packages: tensorboard-plugin-wit, keras, flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, gast, google-auth-oauthlib, tensorboard, tensorflow-gpu\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 23.5.26\n",
            "    Uninstalling flatbuffers-23.5.26:\n",
            "      Successfully uninstalled flatbuffers-23.5.26\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.15.0\n",
            "    Uninstalling tensorflow-estimator-2.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.4\n",
            "    Uninstalling gast-0.5.4:\n",
            "      Successfully uninstalled gast-0.5.4\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.0\n",
            "    Uninstalling google-auth-oauthlib-1.2.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.1\n",
            "    Uninstalling tensorboard-2.15.1:\n",
            "      Successfully uninstalled tensorboard-2.15.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow 2.15.0 requires flatbuffers>=23.5.26, but you have flatbuffers 1.12 which is incompatible.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 2.9.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow 2.15.0 requires tensorboard<2.16,>=2.15, but you have tensorboard 2.9.1 which is incompatible.\n",
            "tensorflow 2.15.0 requires tensorflow-estimator<2.16,>=2.15.0, but you have tensorflow-estimator 2.9.0 which is incompatible.\n",
            "tensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.9.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-estimator-2.9.0 tensorflow-gpu-2.9.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install tensorflow-gpu==2.9.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GmA90Ti5IQZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Required Libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import utils,  optimizers, losses\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras import Sequential\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers import Flatten,Dense,BatchNormalization,Activation,Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import osqp\n",
        "import tensorflow\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from keras import Input\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import pandas as pd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arDHHm_bEwQK",
        "outputId": "986477bb-43bb-47cf-8d23-a7e9f0f15669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "m7RbOSWh-J10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu==2.9.1"
      ],
      "metadata": {
        "id": "f0Kbt1JU-TfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6WJvBV82-lCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCOZs0217LQF"
      },
      "outputs": [],
      "source": [
        "fpath = \"/content/drive/MyDrive/project2/archive (4)/PlantVillage\"\n",
        "random_seed = 111\n",
        "\n",
        "categories = os.listdir(fpath)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(fpath)"
      ],
      "metadata": {
        "id": "LjmmELd_-pbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_and_labels(categories):\n",
        "    img_lst=[]\n",
        "    labels=[]\n",
        "    for index, category in enumerate(categories):\n",
        "        for image_name in os.listdir(fpath+\"/\"+category)[:1000]:\n",
        "            file_ext = image_name.split(\".\")[-1]\n",
        "            if (file_ext.lower() == \"jpg\") or (file_ext.lower() == \"jpeg\"):\n",
        "                img = cv2.imread(fpath+\"/\"+category+\"/\"+image_name)\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                img_array = Image.fromarray(img, 'RGB')\n",
        "                resized_img = img_array.resize((128, 128))\n",
        "                img_lst.append(np.array(resized_img))\n",
        "                labels.append(index)\n",
        "    return img_lst, labels\n",
        "\n",
        "images, labels = load_images_and_labels(categories)\n",
        "print(\"No. of images loaded = \",len(images),\"\\nNo. of labels loaded = \",len(labels))\n",
        "print(type(images),type(labels))"
      ],
      "metadata": {
        "id": "o0ukWA43-u62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting images to numpy array\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "nojUUx35_EOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size = 0.15)\n",
        "\n",
        "print(\"x_train shape = \",x_train.shape)\n",
        "print(\"y_train shape = \",y_train.shape)\n",
        "print(\"\\nx_test shape = \",x_test.shape)\n",
        "print(\"y_test shape = \",y_test.shape)"
      ],
      "metadata": {
        "id": "sW5tvKMT_JIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp2FauRMDNqL",
        "outputId": "538027a9-a43a-411d-dc10-a16b147edde4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape =  (3779, 128, 128, 3)\n",
            "y_train shape =  (3779,)\n",
            "\n",
            "x_val shape =  (1620, 128, 128, 3)\n",
            "y_val shape =  (1620,)\n"
          ]
        }
      ],
      "source": [
        "x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=0.3)\n",
        "\n",
        "print(\"x_train shape = \",x_train.shape)\n",
        "print(\"y_train shape = \",y_train.shape)\n",
        "print(\"\\nx_val shape = \",x_val.shape)\n",
        "print(\"y_val shape = \",y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1t5IdpELSH4b"
      },
      "outputs": [],
      "source": [
        "# Create Model Structure\n",
        "img_size = (224, 224)\n",
        "channels = 3\n",
        "img_shape = (img_size[0], img_size[1], channels)\n",
        "class_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer\n",
        "\n",
        "# we will use EfficientNetB0 from EfficientNet family.\n",
        "base_model = tf.keras.applications.efficientnet.EfficientNetB0(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 ),\n",
        "    Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n",
        "                bias_regularizer=regularizers.l1(0.006) ,activation='relu'),\n",
        "    Dropout(rate=.4, seed=42),\n",
        "    Dense(class_count, activation= 'softmax')\n",
        "])\n",
        "\n",
        "model.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p-ockUi9C8g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8_leaYBCQZN",
        "outputId": "e7b17382-5963-44e7-c4a8-13b026999111"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-d4f2e33d4158>:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(train_generatorENB0.flow(x_train, y_train, batch_size = batch_size), epochs = 10, steps_per_epoch = 236, validation_data = val_generatorENB0.flow(x_val, y_val, batch_size = batch_size), validation_steps = 101, verbose = 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "236/236 [==============================] - 74s 122ms/step - loss: 0.3127 - accuracy: 0.8926 - val_loss: 0.1061 - val_accuracy: 0.9666\n",
            "Epoch 2/10\n",
            "236/236 [==============================] - 27s 113ms/step - loss: 0.1470 - accuracy: 0.9487 - val_loss: 0.0889 - val_accuracy: 0.9759\n",
            "Epoch 3/10\n",
            "236/236 [==============================] - 26s 111ms/step - loss: 0.1131 - accuracy: 0.9591 - val_loss: 0.0739 - val_accuracy: 0.9821\n",
            "Epoch 4/10\n",
            "236/236 [==============================] - 27s 114ms/step - loss: 0.1326 - accuracy: 0.9543 - val_loss: 0.1005 - val_accuracy: 0.9629\n",
            "Epoch 5/10\n",
            "236/236 [==============================] - 24s 103ms/step - loss: 0.1233 - accuracy: 0.9554 - val_loss: 0.0959 - val_accuracy: 0.9691\n",
            "Epoch 6/10\n",
            "236/236 [==============================] - 25s 107ms/step - loss: 0.0913 - accuracy: 0.9668 - val_loss: 0.0600 - val_accuracy: 0.9821\n",
            "Epoch 7/10\n",
            "236/236 [==============================] - 26s 111ms/step - loss: 0.0945 - accuracy: 0.9644 - val_loss: 0.0762 - val_accuracy: 0.9827\n",
            "Epoch 8/10\n",
            "236/236 [==============================] - 27s 112ms/step - loss: 0.1040 - accuracy: 0.9663 - val_loss: 0.0719 - val_accuracy: 0.9802\n",
            "Epoch 9/10\n",
            "236/236 [==============================] - 26s 110ms/step - loss: 0.0836 - accuracy: 0.9713 - val_loss: 0.0850 - val_accuracy: 0.9728\n",
            "Epoch 10/10\n",
            "236/236 [==============================] - 25s 107ms/step - loss: 0.0860 - accuracy: 0.9742 - val_loss: 0.0955 - val_accuracy: 0.9740\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fe59a688a60>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "#Training the model\n",
        "model.fit_generator(train_generatorENB0.flow(x_train, y_train, batch_size = batch_size), epochs = 10, steps_per_epoch = 236, validation_data = val_generatorENB0.flow(x_val, y_val, batch_size = batch_size), validation_steps = 101, verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[INFO] Calculating model accuracy\")\n",
        "scores = model.evaluate(X_test, Y_test)\n",
        "print(f\"Test Accuracy: {scores[1]*100}\")"
      ],
      "metadata": {
        "id": "_9SbstqGEJSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Calculating model accuracy\n",
        "1/1 [==============================] - 0s 95ms/step - loss: 0.0000e+00\n",
        " accuracy: 98.0500"
      ],
      "metadata": {
        "id": "0LyLNuKTFGJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_files, train_labels = define_paths(\"/content/drive/MyDrive/project2/archive (4)/PlantVillage/train\")"
      ],
      "metadata": {
        "id": "RUqHbeekHIXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_files, valid_labels = define_paths(\"/content/drive/MyDrive/project2/archive (4)/PlantVillage/test\")"
      ],
      "metadata": {
        "id": "AyBFRSLWHZE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_df(files, classes):\n",
        "    Fseries = pd.Series(files, name= 'filepaths')\n",
        "    Lseries = pd.Series(classes, name='labels')\n",
        "    return pd.concat([Fseries, Lseries], axis= 1)"
      ],
      "metadata": {
        "id": "Xs7C3vTZHx6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = define_df(train_files, train_labels)"
      ],
      "metadata": {
        "id": "2TiSOSZYH2gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f135c40-5233-465b-89c1-12876c3aa872",
        "id": "WPpq0BC2H80Q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1278 entries, 0 to 1277\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   filepaths  1278 non-null   object\n",
            " 1   labels     1278 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 20.1+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(trn_acc_list, label='train_acc')\n",
        "plt.plot(val_acc_list, label='val_acc')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "5okW1WdGH-EU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}